import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras import backend as K
import tensorflow as tf
import gc, os

# -----------------------------
# 1ï¸âƒ£ Táº¡o sequence cho LSTM
# -----------------------------
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length), :])
        y.append(data[i + seq_length, 0])  # Dá»± Ä‘oÃ¡n cá»™t last_price
    return np.array(X), np.array(y)

# -----------------------------
# 2ï¸âƒ£ HÃ m chÃ­nh
# -----------------------------
def daily_learning_prediction():
    # Káº¿t ná»‘i Spark
    spark = SparkSession.builder \
        .appName("StockML_LSTM_Predict") \
        .config("spark.jars.packages", "org.postgresql:postgresql:42.2.23") \
        .config("spark.driver.memory", "6g") \
        .getOrCreate()

    db_url = "jdbc:postgresql://localhost:5432/stock_ml"
    db_props = {
        "user": "bazzi",
        "password": "bazzi123",
        "driver": "org.postgresql.Driver"
    }

    # -----------------------------
    # 3ï¸âƒ£ Äá»c dá»¯ liá»‡u feature tá»« DB
    # -----------------------------
    print("ğŸ“¥ Äang Ä‘á»c dá»¯ liá»‡u feature tá»« DB stock_ml.feature ...")
    feature_df = spark.read.jdbc(url=db_url, table="ml_data.feature_data", properties=db_props)
    feature_df = feature_df.filter(col("tradingdate").isNotNull())
    pandas_df = feature_df.toPandas()

    pandas_df['tradingdate'] = pd.to_datetime(pandas_df['tradingdate'])
    pandas_df = pandas_df.sort_values(by=['symbol', 'tradingdate']).reset_index(drop=True)

    features = [
        'last_price', 'avg_price', 'ref_price', 'total_val',
        'change', 'ratio_change', 'highest', 'lowest'
    ]

    print(f"ğŸ”¹ Sá»‘ lÆ°á»£ng symbol: {pandas_df['symbol'].nunique()}")
    all_predictions = pd.DataFrame()

    # -----------------------------
    # 4ï¸âƒ£ Huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n tá»«ng mÃ£
    # -----------------------------
    for symbol in pandas_df['symbol'].unique():
        symbol_df = pandas_df[pandas_df['symbol'] == symbol].copy()
        print(f"\nğŸš€ Symbol: {symbol} ({len(symbol_df)} báº£n ghi)")

        if len(symbol_df) < 5:
            print(f"âš ï¸ {symbol}: Dá»¯ liá»‡u quÃ¡ Ã­t, bá» qua.")
            continue

        # Chuáº©n hÃ³a dá»¯ liá»‡u
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(symbol_df[features].values)

        seq_length = 3
        X, y = create_sequences(scaled_data, seq_length)

        if len(X) == 0:
            print(f"âš ï¸ {symbol}: KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ táº¡o sequence.")
            continue

        # Huáº¥n luyá»‡n theo tá»«ng ngÃ y (incremental)
        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        model = Sequential([
            LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),
            Dropout(0.1),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mean_squared_error')
        model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=0)

        # Dá»± Ä‘oÃ¡n test set
        preds = model.predict(X_test)
        preds_full = np.zeros((len(preds), scaled_data.shape[1]))
        preds_full[:, 0] = preds.flatten()
        pred_prices = scaler.inverse_transform(preds_full)[:, 0]

        y_test_full = np.zeros((len(y_test), scaled_data.shape[1]))
        y_test_full[:, 0] = y_test.flatten()
        true_prices = scaler.inverse_transform(y_test_full)[:, 0]

        mse = mean_squared_error(true_prices, pred_prices)
        print(f"âœ… {symbol}: MSE = {mse:.6f}")

        # Dá»± Ä‘oÃ¡n giÃ¡ ngÃ y tiáº¿p theo (next-day prediction)
        last_seq = scaled_data[-seq_length:]
        next_scaled = model.predict(np.expand_dims(last_seq, axis=0))
        next_full = np.zeros((1, scaled_data.shape[1]))
        next_full[:, 0] = next_scaled.flatten()
        predicted_next_day_price = scaler.inverse_transform(next_full)[:, 0][0]

        latest_date = symbol_df['tradingdate'].iloc[-1]
        latest_price = symbol_df['last_price'].iloc[-1]

        all_predictions = pd.concat([all_predictions, pd.DataFrame([{
            'symbol': symbol,
            'tradingdate': latest_date.strftime("%Y-%m-%d"),
            'last_price': round(latest_price, 2),
            'predicted_next_day_price': round(predicted_next_day_price, 2)
        }])], ignore_index=True)

        # Giáº£i phÃ³ng bá»™ nhá»›
        K.clear_session()
        gc.collect()

    # -----------------------------
    # 5ï¸âƒ£ LÆ°u káº¿t quáº£ vÃ o DB target
    # -----------------------------
    print("\nğŸ’¾ LÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ o báº£ng stock_ml.target ...")

    spark_target_df = spark.createDataFrame(all_predictions)
    spark_target_df.write.jdbc(
        url=db_url,
        table="ml_data.target_data",
        mode="append",   # Ghi thÃªm tá»«ng batch káº¿t quáº£
        properties=db_props
    )

    print("âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng vÃ o stock_ml.target")
    spark.stop()


# -----------------------------
# 6ï¸âƒ£ Cháº¡y chÃ­nh
# -----------------------------
if __name__ == "__main__":
    daily_learning_prediction()
